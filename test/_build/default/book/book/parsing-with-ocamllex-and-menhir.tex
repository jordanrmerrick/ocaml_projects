\hypertarget{parsing-with-ocamllex-and-menhir}{%
\section{Parsing with OCamllex and
Menhir}\label{parsing-with-ocamllex-and-menhir}}

Many programming tasks start with the interpretion of some form of
structured textual data. \emph{Parsing} is the process of converting
such data into data structures that are easy to program against. For
simple formats, it's often enough to parse the data in an ad hoc way,
say, by breaking up the data into lines, and then using regular
expressions for breaking those lines down into their component pieces.

But this simplistic approach tends to fall down when parsing more
complicated data, particularly data with the kind of recursive structure
you find in full-blown programming languages or flexible data formats
like JSON and XML. Parsing such formats accurately and efficiently while
providing useful error messages is a complex task.

Often, you can find an existing parsing library that handles these
issues for you. But there are tools to simplify the task when you do
need to write a parser, in the form of \emph{parser generators}. A
parser generator creates a parser from a specification of the data
format that you want to parse, and uses that to generate a parser.
\index{parsing/parser generators}

Parser generators have a long history, including tools like
\passthrough{\lstinline!lex!} and \passthrough{\lstinline!yacc!} that
date back to the early 1970s. OCaml has its own alternatives, including
\passthrough{\lstinline!ocamllex!}, which replaces
\passthrough{\lstinline!lex!}, and \passthrough{\lstinline!ocamlyacc!}
and \passthrough{\lstinline!menhir!}, which replace
\passthrough{\lstinline!yacc!}. We'll explore these tools in the course
of walking through the implementation of a parser for the JSON
serialization format that we discussed in
\href{json.html\#handling-json-data}{Handling Json Data}.

Parsing is a broad and often intricate topic, and our purpose here is
not to teach all of the theoretical issues, but to provide a pragmatic
introduction of how to build a parser in OCaml.
\index{ocamlyacc parser generator}\index{Menhir
parser generator/vs. ocamlyacc}

\hypertarget{menhir-versus-ocamlyacc}{%
\subsection{Menhir Versus ocamlyacc}\label{menhir-versus-ocamlyacc}}

Menhir is an alternative parser generator that is generally superior to
the venerable \passthrough{\lstinline!ocamlyacc!}, which dates back
quite a few years. Menhir is mostly compatible with
\passthrough{\lstinline!ocamlyacc!} grammars, and so you can usually
just switch to Menhir and expect older code to work (with some minor
differences described in the Menhir manual).

The biggest advantage of Menhir is that its error messages are generally
more human-comprehensible, and the parsers that it generates are fully
reentrant and can be parameterized in OCaml modules more easily. We
recommend that any new code you develop should use Menhir instead of
\passthrough{\lstinline!ocamlyacc!}.

Menhir isn't distributed directly with OCaml but is available through
OPAM by running \passthrough{\lstinline!opam install menhir!}.

\hypertarget{lexing-and-parsing}{%
\subsection{Lexing and Parsing}\label{lexing-and-parsing}}

Parsing is traditionally broken down into two parts: \emph{lexical
analysis}, which is a kind of simplified parsing phase that converts a
stream of characters into a stream of logical tokens; and full-on
parsing, which involves converting a stream of tokens into the final
representation, which is often in the form of a tree-like data structure
called an \emph{abstract syntax tree}, or AST.
\index{AST (abstract syntax-tree)}\index{lexical
analysis (lexing)}

It's confusing that the term parsing is applied to both the overall
process of converting textual data to structured data, and also more
specifically to the second phase of converting a stream of tokens to an
AST; so from here on out, we'll use the term parsing to refer only to
this second phase.

Let's consider lexing and parsing in the context of the JSON format.
Here's a snippet of text that represents a JSON object containing a
string labeled \passthrough{\lstinline!title!} and an array containing
two objects, each with a name and array of zip codes:

\begin{lstlisting}
{
  "title": "Cities",
  "cities": [
    { "name": "Chicago",  "zips": [60601] },
    { "name": "New York", "zips": [10004] }
  ]
}
\end{lstlisting}

At a syntactic level, we can think of a JSON file as a series of simple
logical units, like curly braces, square brackets, commas, colons,
identifiers, numbers, and quoted strings. Thus, we could represent our
JSON text as a sequence of tokens of the following type:

\begin{lstlisting}[language=Caml]
type token =
  | NULL
  | TRUE
  | FALSE
  | STRING of string
  | INT of int
  | FLOAT of float
  | ID of string
  | LEFT_BRACK
  | RIGHT_BRACK
  | LEFT_BRACE
  | RIGHT_BRACE
  | COMMA
  | COLON
  | EOF
\end{lstlisting}

Note that this representation loses some information about the original
text. For example, whitespace is not represented. It's common, and
indeed useful, for the token stream to forget some details of the
original text that are not required for understanding its meaning.

If we converted the preceding example into a list of these tokens, it
would look something like this:

\begin{lstlisting}[language=Caml]
[ LEFT_BRACE; ID("title"); COLON; STRING("Cities"); COMMA; ID("cities"); ...
\end{lstlisting}

This kind of representation is easier to work with than the original
text, since it gets rid of some unimportant syntactic details and adds
useful structure. But it's still a good deal more low-level than the
simple AST we used for representing JSON data in
\href{json.html\#handling-json-data}{Handling Json Data}:

\begin{lstlisting}[language=Caml]
type value = [
  | `Assoc of (string * value) list
  | `Bool of bool
  | `Float of float
  | `Int of int
  | `List of value list
  | `Null
  | `String of string
]

(* part 1 *)
open Core
open Out_channel

let rec output_value outc = function
  | `Assoc obj  -> print_assoc outc obj
  | `List l     -> print_list outc l
  | `String s   -> printf "\"%s\"" s
  | `Int i      -> printf "%d" i
  | `Float x    -> printf "%f" x
  | `Bool true  -> output_string outc "true"
  | `Bool false -> output_string outc "false"
  | `Null       -> output_string outc "null"

and print_assoc outc obj =
  output_string outc "{ ";
  let sep = ref "" in
  List.iter ~f:(fun (key, value) ->
      printf "%s\"%s\": %a" !sep key output_value value;
      sep := ",\n  ") obj;
  output_string outc " }"

and print_list outc arr =
  output_string outc "[";
  List.iteri ~f:(fun i v ->
      if i > 0 then
        output_string outc ", ";
      output_value outc v) arr;
  output_string outc "]"
\end{lstlisting}

This representation is much richer than our token stream, capturing the
fact that JSON values can be nested inside each other and that JSON has
a variety of value types, including numbers, strings, arrays, and
objects. The parser we'll write will convert a token stream into a value
of this AST type, as shown below for our earlier JSON example:

\begin{lstlisting}[language=Caml]
`Assoc
  ["title", `String "Cities";
   "cities", `List
     [`Assoc ["name", `String "Chicago"; "zips", `List [`Int 60601]];
      `Assoc ["name", `String "New York"; "zips", `List [`Int 10004]]]]
\end{lstlisting}

\hypertarget{defining-a-parser}{%
\subsection{Defining a Parser}\label{defining-a-parser}}

A parser-specification file has suffix \passthrough{\lstinline!.mly!}
and contains two sections that are broken up by separator lines
consisting of the characters \passthrough{\lstinline!\%\%!} on a line by
themselves. The first section of the file is for declarations, including
token and type specifications, precedence directives, and other output
directives; and the second section is for specifying the grammar of the
language to be parsed.
\index{files/mly files}\protect\hypertarget{PARSparsdef}{}{parsing/parser
definition}

We'll start by declaring the list of tokens. A token is declared using
the syntax
\passthrough{\lstinline!\%token <!}\emph{\passthrough{\lstinline!type!}}\passthrough{\lstinline!>!}\emph{\passthrough{\lstinline!uid!}},
where the \emph{\passthrough{\lstinline!<type>!}} is optional and
\emph{\passthrough{\lstinline!uid!}} is a capitalized identifier. For
JSON, we need tokens for numbers, strings, identifiers, and punctuation:
\index{tokens, declaration of}

\begin{lstlisting}[language=Caml]
%token <int> INT
%token <float> FLOAT
%token <string> ID
%token <string> STRING
%token TRUE
%token FALSE
%token NULL
%token LEFT_BRACE
%token RIGHT_BRACE
%token LEFT_BRACK
%token RIGHT_BRACK
%token COLON
%token COMMA
%token EOF
\end{lstlisting}

The
\passthrough{\lstinline!<!}\emph{\passthrough{\lstinline!type!}}\passthrough{\lstinline!>!}
specifications mean that a token carries a value. The
\passthrough{\lstinline!INT!} token carries an integer value with it,
\passthrough{\lstinline!FLOAT!} has a \passthrough{\lstinline!float!}
value, and \passthrough{\lstinline!STRING!} carries a
\passthrough{\lstinline!string!} value. The remaining tokens, such as
\passthrough{\lstinline!TRUE!}, \passthrough{\lstinline!FALSE!}, or the
punctuation, aren't associated with any value, and so we can omit the
\passthrough{\lstinline!<!}\emph{\passthrough{\lstinline!type!}}\passthrough{\lstinline!>!}
specification.

\hypertarget{describing-the-grammar}{%
\subsubsection{Describing the Grammar}\label{describing-the-grammar}}

The next thing we need to do is to specify the grammar of a JSON
expression. \passthrough{\lstinline!menhir!}, like many parser
generators, expresses grammars as \emph{context-free grammars}. (More
precisely, \passthrough{\lstinline!menhir!} supports LR(1) grammars, but
we will ignore that technical distinction here.) You can think of a
context-free grammar as a set of abstract names, called
\emph{non-terminal symbols}, along with a collection of rules for
transforming a nonterminal symbol into a sequence of tokens and
nonterminal symbols. A sequence of tokens is parsable by a grammar if
you can apply the grammar's rules to produce a series of
transformations, starting at a distinguished \emph{start symbol} that
produces the token sequence in {question}.
\index{grammars/context-free}\index{LR(1) grammars}\index{start
symbols}\index{non-terminal symbols}\index{context-free
grammars}\index{Menhir parser generator/context-free grammars in}

We'll start describing the JSON grammar by declaring the start symbol to
be the non-terminal symbol \passthrough{\lstinline!prog!}, and by
declaring that when parsed, a \passthrough{\lstinline!prog!} value
should be converted into an OCaml value of type
\passthrough{\lstinline!Json.value option!}. We then end the declaration
section of the parser with a \passthrough{\lstinline!\%\%!}:

\begin{lstlisting}[language=Caml]
%start <Json.value option> prog
%%
\end{lstlisting}

Once that's in place, we can start specifying the productions. In
\passthrough{\lstinline!menhir!}, productions are organized into
\emph{rules}, where each rule lists all the possible productions for a
given nonterminal symbols. Here, for example, is the rule for
\passthrough{\lstinline!prog!}:

\begin{lstlisting}[language=Caml]
prog:
  | EOF       { None }
  | v = value { Some v }
  ;
\end{lstlisting}

The syntax for this is reminiscent of an OCaml
\passthrough{\lstinline!match!} statement. The pipes separate the
individual productions, and the curly braces contain a \emph{semantic
action}: OCaml code that generates the OCaml value corresponding to the
production in question. Semantic actions are arbitrary OCaml expressions
that are evaluated during parsing to produce values that are attached to
the non-terminal in the rule.
\index{semantic actions}\index{curly braces (\{ \})}

We have two cases for \passthrough{\lstinline!prog!}: either there's an
\passthrough{\lstinline!EOF!}, which means the text is empty, and so
there's no JSON value to read, we return the OCaml value
\passthrough{\lstinline!None!}; or we have an instance of the
\passthrough{\lstinline!value!} nonterminal, which corresponds to a
well-formed JSON value, and we wrap the corresponding
\passthrough{\lstinline!Json.value!} in a \passthrough{\lstinline!Some!}
tag. Note that in the \passthrough{\lstinline!value!} case, we wrote
\passthrough{\lstinline!v = value!} to bind the OCaml value that
corresponds to the variable \passthrough{\lstinline!v!}, which we can
then use within the curly braces for that production.

Now let's consider a more complex example, the rule for the
\passthrough{\lstinline!value!} symbol:

\begin{lstlisting}[language=Caml]
value:
  | LEFT_BRACE; obj = object_fields; RIGHT_BRACE
    { `Assoc obj }
  | LEFT_BRACK; vl = array_values; RIGHT_BRACK
    { `List vl }
  | s = STRING
    { `String s }
  | i = INT
    { `Int i }
  | x = FLOAT
    { `Float x }
  | TRUE
    { `Bool true }
  | FALSE
    { `Bool false }
  | NULL
    { `Null }
  ;
\end{lstlisting}

According to these rules, a JSON \passthrough{\lstinline!value!} is
either: \index{values/in JSON
data}

\begin{itemize}
\item
  An object bracketed by curly braces
\item
  An array bracketed by square braces
\item
  A string, integer, float, bool, or null value
\end{itemize}

In each of the productions, the OCaml code in curly braces shows what to
transform the object in question to. Note that we still have two
nonterminals whose definitions we depend on here but have not yet
defined: \passthrough{\lstinline!object\_fields!} and
\passthrough{\lstinline!array\_values!}. We'll look at how these are
parsed next.

\hypertarget{parsing-sequences}{%
\subsubsection{Parsing Sequences}\label{parsing-sequences}}

The rule for \passthrough{\lstinline!object\_fields!} follows, and is
really just a thin wrapper that reverses the list returned by the
following rule for \passthrough{\lstinline!rev\_object\_fields!}. Note
that the first production in
\passthrough{\lstinline!rev\_object\_fields!} has an empty left-hand
side, because what we're matching on in this case is an empty sequence
of tokens. The comment \passthrough{\lstinline!(* empty *)!} is used to
make this clear: \index{rev\_object\_fields}\index{object\_fields}

\begin{lstlisting}[language=Caml]
object_fields: obj = rev_object_fields { List.rev obj };

rev_object_fields:
  | (* empty *) { [] }
  | obj = rev_object_fields; COMMA; k = ID; COLON; v = value
    { (k, v) :: obj }
  ;
\end{lstlisting}

The rules are structured as they are because
\passthrough{\lstinline!menhir!} generates left-recursive parsers, which
means that the constructed pushdown automaton uses less stack space with
left-recursive definitions. The following right-recursive rule accepts
the same input, but during parsing, it requires linear stack space to
read object field definitions: \index{Menhir parser
generator/left-recursive definitions}

\begin{lstlisting}[language=Caml]
(* Inefficient right-recursive rule *)
object_fields:
  | (* empty *) { [] }
  | k = ID; COLON; v = value; COMMA; obj = object_fields
    { (k, v) :: obj }
\end{lstlisting}

Alternatively, we could keep the left-recursive definition and simply
construct the returned value in left-to-right order. This is even less
efficient, since the complexity of building the list incrementally in
this way is quadratic in the length of the list:

\begin{lstlisting}[language=Caml]
(* Quadratic left-recursive rule *)
object_fields:
  | (* empty *) { [] }
  | obj = object_fields; COMMA; k = ID; COLON; v = value
    { obj @ [k, v] }
  ;
\end{lstlisting}

Assembling lists like this is a pretty common requirement in most
realistic grammars, and the preceding rules (while useful for
illustrating how parsing works) are rather verbose. Menhir features an
extended standard library of built-in rules to simplify this handling.
These rules are detailed in the Menhir manual and include optional
values, pairs of values with optional separators, and lists of elements
(also with optional separators). \index{Menhir
parser generator/built-in rules of}

A version of the JSON grammar using these more succinct Menhir rules
follows. Notice the use of \passthrough{\lstinline!separated\_list!} to
parse both JSON objects and lists with one rule:

\begin{lstlisting}[language=Caml]
prog:
  | v = value { Some v }
  | EOF       { None   } ;

value:
  | LEFT_BRACE; obj = obj_fields; RIGHT_BRACE { `Assoc obj  }
  | LEFT_BRACK; vl = list_fields; RIGHT_BRACK { `List vl    }
  | s = STRING                                { `String s   }
  | i = INT                                   { `Int i      }
  | x = FLOAT                                 { `Float x    }
  | TRUE                                      { `Bool true  }
  | FALSE                                     { `Bool false }
  | NULL                                      { `Null       } ;

obj_fields:
    obj = separated_list(COMMA, obj_field)    { obj } ;

obj_field:
    k = STRING; COLON; v = value              { (k, v) } ;

list_fields:
    vl = separated_list(COMMA, value)         { vl } ;
\end{lstlisting}

We can invoke \passthrough{\lstinline!menhir!} by using
\passthrough{\lstinline!corebuild!} with the
\passthrough{\lstinline!-use-menhir!} flag. This tells the build system
to switch to using \passthrough{\lstinline!menhir!} instead of
\passthrough{\lstinline!ocamlyacc!} to handle files with the
\passthrough{\lstinline!.mly!} suffix: {-use-menhir
flag}\index{Menhir parser
generator/invoking}~

\begin{lstlisting}
(rule
 (targets short_parser.mli short_parser.ml)
  (deps   short_parser.mly)
  (action (ignore-stderr (run menhir --external-tokens Json --explain ${<}))))
\end{lstlisting}

\begin{lstlisting}[language=bash]
\end{lstlisting}

\hypertarget{defining-a-lexer}{%
\subsection{Defining a Lexer}\label{defining-a-lexer}}

Now we can define a lexer, using \passthrough{\lstinline!ocamllex!}, to
convert our input text into a stream of tokens. The specification of the
lexer is placed in a file with an \passthrough{\lstinline!.mll!} suffix.
\index{lexers/specification of}\index{OCaml
toolchain/ocamllex}\index{mll files}\index{files/mll
files}\protect\hypertarget{PARlex}{}{parsing/lexer definition}

\hypertarget{ocaml-prelude}{%
\subsubsection{OCaml Prelude}\label{ocaml-prelude}}

Let's walk through the definition of a lexer section by section. The
first section is an optional chunk of OCaml code that is bounded by a
pair of curly braces: \index{lexers/optional OCaml code for}

\begin{lstlisting}[language=Caml]
{
open Lexing
open Parser

exception SyntaxError of string

let next_line lexbuf =
  let pos = lexbuf.lex_curr_p in
  lexbuf.lex_curr_p <-
    { pos with pos_bol = lexbuf.lex_curr_pos;
               pos_lnum = pos.pos_lnum + 1
    }
}
\end{lstlisting}

This code is there to define utility functions used by later snippets of
OCaml code and to set up the environment by opening useful modules and
define an exception, \passthrough{\lstinline!SyntaxError!}.

We also define a utility function \passthrough{\lstinline!next\_line!}
for tracking the location of tokens across line breaks. The
\passthrough{\lstinline!Lexing!} module defines a
\passthrough{\lstinline!lexbuf!} structure that holds the state of the
lexer, including the current location within the source file. The
\passthrough{\lstinline!next\_line!} function simply accesses the
\passthrough{\lstinline!lex\_curr\_p!} field that holds the current
location and updates its line number.

\hypertarget{regular-expressions}{%
\subsubsection{Regular Expressions}\label{regular-expressions}}

The next section of the lexing file is a collection of named regular
expressions. These look syntactically like ordinary OCaml
\passthrough{\lstinline!let!} bindings, but really this is a specialized
syntax for declaring regular expressions. Here's an example:
\index{regular expressions}\index{lexers/regular expressions
collection}

\begin{lstlisting}[language=Caml]
let int = '-'? ['0'-'9'] ['0'-'9']*
\end{lstlisting}

The syntax here is something of a hybrid between OCaml syntax and
traditional regular expression syntax. The \passthrough{\lstinline!int!}
regular expression specifies an optional leading
\passthrough{\lstinline!-!}, followed by a digit from
\passthrough{\lstinline!0!} to \passthrough{\lstinline!9!}, followed by
some number of digits from \passthrough{\lstinline!0!} to
\passthrough{\lstinline!9!}. The question mark is used to indicate an
optional component of a regular expression; the square brackets are used
to specify ranges; and the \passthrough{\lstinline!*!} operator is used
to indicate a (possibly empty) repetition.

Floating-point numbers are specified similarly, but we deal with decimal
points and exponents. We make the expression easier to read by building
up a sequence of named regular expressions, rather than creating one big
and impenetrable expression:

\begin{lstlisting}[language=Caml]
let digit = ['0'-'9']
let frac = '.' digit*
let exp = ['e' 'E'] ['-' '+']? digit+
let float = digit* frac? exp?
\end{lstlisting}

Finally, we define whitespace, newlines, and identifiers:

\begin{lstlisting}[language=Caml]
let white = [' ' '\t']+
let newline = '\r' | '\n' | "\r\n"
let id = ['a'-'z' 'A'-'Z' '_'] ['a'-'z' 'A'-'Z' '0'-'9' '_']*
\end{lstlisting}

The \passthrough{\lstinline!newline!} introduces the
\passthrough{\lstinline!|!} operator, which lets one of several
alternative regular expressions match (in this case, the various
carriage-return combinations of CR, LF, or CRLF).

\hypertarget{lexing-rules}{%
\subsubsection{Lexing Rules}\label{lexing-rules}}

The lexing rules are essentially functions that consume the data,
producing OCaml expressions that evaluate to tokens. These OCaml
expressions can be quite complicated, using side effects and invoking
other rules as part of the body of the rule. Let's look at the
\passthrough{\lstinline!read!} rule for parsing a JSON expression:
\index{lexers/rules for}

\begin{lstlisting}[language=Caml]
rule read =
  parse
  | white    { read lexbuf }
  | newline  { next_line lexbuf; read lexbuf }
  | int      { INT (int_of_string (Lexing.lexeme lexbuf)) }
  | float    { FLOAT (float_of_string (Lexing.lexeme lexbuf)) }
  | "true"   { TRUE }
  | "false"  { FALSE }
  | "null"   { NULL }
  | '"'      { read_string (Buffer.create 17) lexbuf }
  | '{'      { LEFT_BRACE }
  | '}'      { RIGHT_BRACE }
  | '['      { LEFT_BRACK }
  | ']'      { RIGHT_BRACK }
  | ':'      { COLON }
  | ','      { COMMA }
  | _ { raise (SyntaxError ("Unexpected char: " ^ Lexing.lexeme lexbuf)) }
  | eof      { EOF }
\end{lstlisting}

The rules are structured very similarly to pattern matches, except that
the variants are replaced by regular expressions on the left-hand side.
The righthand-side clause is the parsed OCaml return value of that rule.
The OCaml code for the rules has a parameter called
\passthrough{\lstinline!lexbuf!} that defines the input, including the
position in the input file, as well as the text that was matched by the
regular expression. \index{pattern matching/vs. lexing rules}

The first \passthrough{\lstinline!white \{ read lexbuf \}!} calls the
lexer recursively. That is, it skips the input whitespace and returns
the following token. The action
\passthrough{\lstinline!newline \{ next\_line lexbuf; read lexbuf \}!}
is similar, but we use it to advance the line number for the lexer using
the utility function that we defined at the top of the file. Let's skip
to the third action:

\begin{lstlisting}[language=Caml]
| int { INT (int_of_string (Lexing.lexeme lexbuf)) }
\end{lstlisting}

This action specifies that when the input matches the
\passthrough{\lstinline!int!} regular expression, then the lexer should
return the expression
\passthrough{\lstinline!INT (int\_of\_string (Lexing.lexeme lexbuf))!}.
The expression \passthrough{\lstinline!Lexing.lexeme lexbuf!} returns
the complete string matched by the regular expression. In this case, the
string represents a number, so we use the
\passthrough{\lstinline!int\_of\_string!} function to convert it to a
number.

There are actions for each different kind of token. The string
expressions like \passthrough{\lstinline!"true" \{ TRUE \}!} are used
for keywords, and the special characters have actions, too, like
\passthrough{\lstinline!'\{' \{ LEFT\_BRACE \}!}.

Some of these patterns overlap. For example, the regular expression
\passthrough{\lstinline!"true"!} is also matched by the
\passthrough{\lstinline!id!} pattern. \passthrough{\lstinline!ocamllex!}
used the following disambiguation when a prefix of the input is matched
by more than one pattern:

\begin{itemize}
\item
  The longest match always wins. For example, the first input
  \passthrough{\lstinline!trueX: 167!} matches the regular expression
  \passthrough{\lstinline!"true"!} for four characters, and it matches
  \passthrough{\lstinline!id!} for five characters. The longer match
  wins, and the return value is \passthrough{\lstinline!ID "trueX"!}.
\item
  If all matches have the same length, then the first action wins. If
  the input were \passthrough{\lstinline!true: 167!}, then both
  \passthrough{\lstinline!"true"!} and \passthrough{\lstinline!id!}
  match the first four characters; \passthrough{\lstinline!"true"!} is
  first, so the return value is \passthrough{\lstinline!TRUE!}.
\end{itemize}

\hypertarget{recursive-rules}{%
\subsubsection{Recursive Rules}\label{recursive-rules}}

Unlike many other lexer generators, \passthrough{\lstinline!ocamllex!}
allows the definition of multiple lexers in the same file, and the
definitions can be recursive. In this case, we use recursion to match
string literals using the following rule definition:
\index{recursion/in lexers}\index{lexers/recursive rules}

\begin{lstlisting}[language=Caml]
and read_string buf =
  parse
  | '"'       { STRING (Buffer.contents buf) }
  | '\\' '/'  { Buffer.add_char buf '/'; read_string buf lexbuf }
  | '\\' '\\' { Buffer.add_char buf '\\'; read_string buf lexbuf }
  | '\\' 'b'  { Buffer.add_char buf '\b'; read_string buf lexbuf }
  | '\\' 'f'  { Buffer.add_char buf '\012'; read_string buf lexbuf }
  | '\\' 'n'  { Buffer.add_char buf '\n'; read_string buf lexbuf }
  | '\\' 'r'  { Buffer.add_char buf '\r'; read_string buf lexbuf }
  | '\\' 't'  { Buffer.add_char buf '\t'; read_string buf lexbuf }
  | [^ '"' '\\']+
    { Buffer.add_string buf (Lexing.lexeme lexbuf);
      read_string buf lexbuf
    }
  | _ { raise (SyntaxError ("Illegal string character: " ^ Lexing.lexeme lexbuf)) }
  | eof { raise (SyntaxError ("String is not terminated")) }
\end{lstlisting}

This rule takes a \passthrough{\lstinline!buf : Buffer.t!} as an
argument. If we reach the terminating double quote
\passthrough{\lstinline!"!}, then we return the contents of the buffer
as a \passthrough{\lstinline!STRING!}.

The other cases are for handling the string contents. The action
\passthrough{\lstinline![^ '"' '\\\\']+ \{ ... \}!} matches normal input
that does not contain a double quote or backslash. The actions beginning
with a backslash \passthrough{\lstinline!\\!} define what to do for
escape sequences. In each of these cases, the final step includes a
recursive call to the lexer.

That covers the lexer. Next, we need to combine the lexer with the
parser to bring it all together.
\index{lexers/Unicode parsing}\index{Uutf Unicode
codec}\index{OCaml toolchain/ocamllex}\index{Ulex lexer
generator}\index{Camomile unicode parser}\index{Unicode, parsing solutions
for}

\hypertarget{handling-unicode}{%
\paragraph{Handling Unicode}\label{handling-unicode}}

We've glossed over an important detail here: parsing Unicode characters
to handle the full spectrum of the world's writing systems. OCaml has
several third-party solutions to handling Unicode, with varying degrees
of flexibility and complexity:

\begin{itemize}
\item
  \href{https://github.com/yoriyuki/Camomile}{Camomile} supports the
  full spectrum of Unicode character types, conversion from around 200
  encodings, and collation and locale-sensitive case mappings.
\item
  \href{http://www.cduce.org/ulex}{Ulex} is a lexer generator for
  Unicode that can serve as a Unicode-aware replacement for
  \passthrough{\lstinline!ocamllex!}.
\item
  \href{http://erratique.ch/software/uutf}{Uutf} is a nonblocking
  streaming Unicode codec for OCaml, available as a standalone library.
  It is accompanied by the
  \href{http://erratique.ch/software/uunf}{Uunf} text normalization and
  \href{http://erratique.ch/software/uucd}{Uucd} Unicode character
  database libraries. There is also a robust parser for
  \href{http://erratique.ch/software/jsonm}{JSON} available that
  illustrates the use of Uutf in your own libraries.
\end{itemize}

All of these libraries are available via OPAM under their respective
names.~

\hypertarget{bringing-it-all-together}{%
\subsection{Bringing It All Together}\label{bringing-it-all-together}}

For the final part, we need to compose the lexer and parser. As we saw
in the type definition in \passthrough{\lstinline!parser.mli!}, the
parsing function expects a lexer of type
\passthrough{\lstinline!Lexing.lexbuf -> token!}, and a
\passthrough{\lstinline!lexbuf!}: \index{parsing/lexer and parser
composition}

\begin{lstlisting}[language=Caml]
val prog:(Lexing.lexbuf -> token) -> Lexing.lexbuf -> Json.value option
\end{lstlisting}

Before we start with the lexing, let's first define some functions to
handle parsing errors. There are currently two errors:
\passthrough{\lstinline!Parser.Error!} and
\passthrough{\lstinline!Lexer.SyntaxError!}. A simple solution when
encountering an error is to print the error and give up:
\index{errors/"give up on first error" approach}

\begin{lstlisting}[language=Caml]
open Core
open Lexer
open Lexing

let print_position outx lexbuf =
  let pos = lexbuf.lex_curr_p in
  fprintf outx "%s:%d:%d" pos.pos_fname
    pos.pos_lnum (pos.pos_cnum - pos.pos_bol + 1)

let parse_with_error lexbuf =
  try Parser.prog Lexer.read lexbuf with
  | SyntaxError msg ->
    fprintf stderr "%a: %s\n" print_position lexbuf msg;
    None
  | Parser.Error ->
    fprintf stderr "%a: syntax error\n" print_position lexbuf;
    exit (-1)
\end{lstlisting}

The ``give up on the first error'' approach is easy to implement but
isn't very friendly. In general, error handling can be pretty intricate,
and we won't discuss it here. However, the Menhir parser defines
additional mechanisms you can use to try and recover from errors. These
are described in detail in its reference
\href{http://gallium.inria.fr/~fpottier/menhir/}{manual}.
\index{Menhir parser
generator/error handling in}

The standard lexing library \passthrough{\lstinline!Lexing!} provides a
function \passthrough{\lstinline!from\_channel!} to read the input from
a channel. The following function describes the structure, where the
\passthrough{\lstinline!Lexing.from\_channel!} function is used to
construct a \passthrough{\lstinline!lexbuf!}, which is passed with the
lexing function \passthrough{\lstinline!Lexer.read!} to the
\passthrough{\lstinline!Parser.prog!} function.
\passthrough{\lstinline!Parsing.prog!} returns
\passthrough{\lstinline!None!} when it reaches end of file. We define a
function \passthrough{\lstinline!Json.output\_value!}, not shown here,
to print a \passthrough{\lstinline!Json.value!}:

\begin{lstlisting}[language=Caml]
let rec parse_and_print lexbuf =
  match parse_with_error lexbuf with
  | Some value ->
    printf "%a\n" Json.output_value value;
    parse_and_print lexbuf
  | None -> ()

let loop filename () =
  let inx = In_channel.create filename in
  let lexbuf = Lexing.from_channel inx in
  lexbuf.lex_curr_p <- { lexbuf.lex_curr_p with pos_fname = filename };
  parse_and_print lexbuf;
  In_channel.close inx

let () =
  Command.basic_spec ~summary:"Parse and display JSON"
    Command.Spec.(empty +> anon ("filename" %: string))
    loop
  |> Command.run
\end{lstlisting}

Here's a test input file we can use to test the code we just wrote:

\begin{lstlisting}
true
false
null
[1, 2, 3., 4.0, .5, 5.5e5, 6.3]
"Hello World"
{ "field1": "Hello",
  "field2": 17e13,
  "field3": [1, 2, 3],
  "field4": { "fieldA": 1, "fieldB": "Hello" }
}
\end{lstlisting}

Now build and run the example using this file, and you can see the full
parser in action:

\begin{lstlisting}[language=bash]
$ dune exec ./test.exe test1.json
true
false
null
[1, 2, 3.000000, 4.000000, 0.500000, 550000.000000, 6.300000]
"Hello World"
{ "field1": "Hello",
  "field2": 170000000000000.000000,
  "field3": [1, 2, 3],
  "field4": { "fieldA": 1,
  "fieldB": "Hello" } }
\end{lstlisting}

With our simple error handling scheme, errors are fatal and cause the
program to terminate with a nonzero exit code:

\begin{lstlisting}[language=bash]
$ cat test2.json
{ "name": "Chicago",
  "zips": [12345,
}
{ "name": "New York",
  "zips": [10004]
}
$ dune exec ./test.exe test2.json
test2.json:3:2: syntax error
[255]
\end{lstlisting}

That wraps up our parsing tutorial. As an aside, notice that the JSON
polymorphic variant type that we defined in this chapter is actually
structurally compatible with the Yojson representation explained in
\href{json.html\#handling-json-data}{Handling Json Data}. That means
that you can take this parser and use it with the helper functions in
Yojson to build more sophisticated applications.
